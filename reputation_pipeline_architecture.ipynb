{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Pipeline Architecture for ML-Based Reputation Management\n",
    "\n",
    "This notebook presents a production-ready, optimized pipeline architecture that integrates all ML components from the reputation management framework. The architecture emphasizes:\n",
    "\n",
    "- **Modularity**: Loosely coupled components with clear interfaces\n",
    "- **Scalability**: Async processing, batching, and parallel execution\n",
    "- **Extensibility**: Plugin-based architecture for new models/data sources\n",
    "- **Observability**: Comprehensive logging, metrics, and monitoring\n",
    "- **Fault Tolerance**: Graceful degradation and error recovery\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         REPUTATION MANAGEMENT PIPELINE                       │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐  │\n",
    "│  │   Data      │    │   Feature   │    │   Model     │    │   Output    │  │\n",
    "│  │  Ingestion  │───▶│ Engineering │───▶│  Ensemble   │───▶│  & Action   │  │\n",
    "│  │   Layer     │    │    Layer    │    │    Layer    │    │    Layer    │  │\n",
    "│  └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘  │\n",
    "│         │                  │                  │                  │         │\n",
    "│         ▼                  ▼                  ▼                  ▼         │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                    Shared Infrastructure Layer                       │   │\n",
    "│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐   │   │\n",
    "│  │  │ Config  │  │ Logging │  │ Metrics │  │  Cache  │  │  State  │   │   │\n",
    "│  │  │ Manager │  │ Service │  │ Tracker │  │  Layer  │  │  Store  │   │   │\n",
    "│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  └─────────┘   │   │\n",
    "│  └─────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import hashlib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('ReputationPipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration Management\n",
    "\n",
    "Centralized configuration with validation and hot-reloading support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for individual models.\"\"\"\n",
    "    name: str\n",
    "    enabled: bool = True\n",
    "    weight: float = 1.0\n",
    "    batch_size: int = 32\n",
    "    device: str = 'cpu'\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Master pipeline configuration.\"\"\"\n",
    "    # Processing settings\n",
    "    max_workers: int = 4\n",
    "    async_enabled: bool = True\n",
    "    batch_size: int = 64\n",
    "    \n",
    "    # Cache settings\n",
    "    cache_enabled: bool = True\n",
    "    cache_ttl_seconds: int = 3600\n",
    "    cache_max_size: int = 10000\n",
    "    \n",
    "    # Anomaly detection\n",
    "    anomaly_threshold: float = 0.5\n",
    "    drift_sensitivity: float = 0.1\n",
    "    \n",
    "    # Alert settings\n",
    "    alert_on_anomaly: bool = True\n",
    "    alert_on_drift: bool = True\n",
    "    min_confidence: float = 0.6\n",
    "    \n",
    "    # Model configurations\n",
    "    sentiment_config: ModelConfig = field(default_factory=lambda: ModelConfig('sentiment'))\n",
    "    anomaly_config: ModelConfig = field(default_factory=lambda: ModelConfig('anomaly'))\n",
    "    forecast_config: ModelConfig = field(default_factory=lambda: ModelConfig('forecast'))\n",
    "    network_config: ModelConfig = field(default_factory=lambda: ModelConfig('network'))\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        assert 0 < self.max_workers <= 32, \"max_workers must be between 1 and 32\"\n",
    "        assert 0 < self.batch_size <= 512, \"batch_size must be between 1 and 512\"\n",
    "        assert 0 <= self.anomaly_threshold <= 1, \"anomaly_threshold must be in [0, 1]\"\n",
    "        return True\n",
    "\n",
    "# Global configuration instance\n",
    "CONFIG = PipelineConfig()\n",
    "CONFIG.validate()\n",
    "print(f\"Pipeline configured with {CONFIG.max_workers} workers, batch_size={CONFIG.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Infrastructure Layer\n",
    "\n",
    "Shared services for caching, metrics, and state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Track pipeline metrics for monitoring and optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics: Dict[str, List[float]] = {}\n",
    "        self.counters: Dict[str, int] = {}\n",
    "        self.timers: Dict[str, List[float]] = {}\n",
    "        \n",
    "    def record(self, name: str, value: float):\n",
    "        \"\"\"Record a metric value.\"\"\"\n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        self.metrics[name].append(value)\n",
    "        \n",
    "    def increment(self, name: str, delta: int = 1):\n",
    "        \"\"\"Increment a counter.\"\"\"\n",
    "        self.counters[name] = self.counters.get(name, 0) + delta\n",
    "        \n",
    "    def time(self, name: str):\n",
    "        \"\"\"Context manager for timing operations.\"\"\"\n",
    "        return TimerContext(self, name)\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get summary statistics.\"\"\"\n",
    "        summary = {'counters': self.counters.copy()}\n",
    "        for name, values in self.metrics.items():\n",
    "            if values:\n",
    "                summary[name] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values),\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values),\n",
    "                    'count': len(values)\n",
    "                }\n",
    "        for name, times in self.timers.items():\n",
    "            if times:\n",
    "                summary[f'{name}_time_ms'] = {\n",
    "                    'mean': np.mean(times) * 1000,\n",
    "                    'p95': np.percentile(times, 95) * 1000,\n",
    "                    'p99': np.percentile(times, 99) * 1000\n",
    "                }\n",
    "        return summary\n",
    "\n",
    "class TimerContext:\n",
    "    def __init__(self, tracker: MetricsTracker, name: str):\n",
    "        self.tracker = tracker\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        elapsed = time.perf_counter() - self.start\n",
    "        if self.name not in self.tracker.timers:\n",
    "            self.tracker.timers[self.name] = []\n",
    "        self.tracker.timers[self.name].append(elapsed)\n",
    "\n",
    "\n",
    "class LRUCache:\n",
    "    \"\"\"LRU Cache with TTL support for caching model predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 10000, ttl_seconds: int = 3600):\n",
    "        self.max_size = max_size\n",
    "        self.ttl = ttl_seconds\n",
    "        self.cache: Dict[str, Tuple[Any, float]] = {}\n",
    "        self.access_order: deque = deque()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        \n",
    "    def _hash_key(self, key: Any) -> str:\n",
    "        \"\"\"Generate hash for cache key.\"\"\"\n",
    "        if isinstance(key, str):\n",
    "            return hashlib.md5(key.encode()).hexdigest()\n",
    "        return hashlib.md5(json.dumps(key, sort_keys=True).encode()).hexdigest()\n",
    "    \n",
    "    def get(self, key: Any) -> Optional[Any]:\n",
    "        \"\"\"Get value from cache.\"\"\"\n",
    "        hkey = self._hash_key(key)\n",
    "        if hkey in self.cache:\n",
    "            value, timestamp = self.cache[hkey]\n",
    "            if time.time() - timestamp < self.ttl:\n",
    "                self.hits += 1\n",
    "                return value\n",
    "            else:\n",
    "                del self.cache[hkey]\n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, key: Any, value: Any):\n",
    "        \"\"\"Set value in cache.\"\"\"\n",
    "        hkey = self._hash_key(key)\n",
    "        self.cache[hkey] = (value, time.time())\n",
    "        self.access_order.append(hkey)\n",
    "        \n",
    "        # Evict if over capacity\n",
    "        while len(self.cache) > self.max_size:\n",
    "            old_key = self.access_order.popleft()\n",
    "            self.cache.pop(old_key, None)\n",
    "    \n",
    "    @property\n",
    "    def hit_rate(self) -> float:\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "# Global instances\n",
    "METRICS = MetricsTracker()\n",
    "CACHE = LRUCache(max_size=CONFIG.cache_max_size, ttl_seconds=CONFIG.cache_ttl_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Layer - Ingestion & Preprocessing\n",
    "\n",
    "Unified data ingestion with support for multiple sources and formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSourceType(Enum):\n",
    "    SOCIAL_MEDIA = \"social_media\"\n",
    "    NEWS = \"news\"\n",
    "    REVIEWS = \"reviews\"\n",
    "    INTERNAL = \"internal\"\n",
    "\n",
    "@dataclass\n",
    "class ReputationDocument:\n",
    "    \"\"\"Standardized document representation.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    source: DataSourceType\n",
    "    timestamp: datetime\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    # Optional multimodal content\n",
    "    image_embedding: Optional[np.ndarray] = None\n",
    "    video_embedding: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Network context\n",
    "    author_id: Optional[str] = None\n",
    "    reply_to: Optional[str] = None\n",
    "    mentions: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class DataIngestionPipeline:\n",
    "    \"\"\"Unified data ingestion with preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.preprocessors: List[Callable] = []\n",
    "        self.logger = logging.getLogger('DataIngestion')\n",
    "        \n",
    "    def add_preprocessor(self, fn: Callable[[ReputationDocument], ReputationDocument]):\n",
    "        \"\"\"Add a preprocessing function.\"\"\"\n",
    "        self.preprocessors.append(fn)\n",
    "        \n",
    "    def preprocess(self, doc: ReputationDocument) -> ReputationDocument:\n",
    "        \"\"\"Apply all preprocessors.\"\"\"\n",
    "        for fn in self.preprocessors:\n",
    "            doc = fn(doc)\n",
    "        return doc\n",
    "    \n",
    "    def ingest_batch(self, documents: List[Dict]) -> List[ReputationDocument]:\n",
    "        \"\"\"Ingest and preprocess a batch of documents.\"\"\"\n",
    "        with METRICS.time('data_ingestion'):\n",
    "            results = []\n",
    "            for doc_dict in documents:\n",
    "                try:\n",
    "                    doc = ReputationDocument(\n",
    "                        id=doc_dict.get('id', str(hash(doc_dict.get('text', '')))),\n",
    "                        text=doc_dict['text'],\n",
    "                        source=DataSourceType(doc_dict.get('source', 'social_media')),\n",
    "                        timestamp=doc_dict.get('timestamp', datetime.now()),\n",
    "                        metadata=doc_dict.get('metadata', {}),\n",
    "                        author_id=doc_dict.get('author_id'),\n",
    "                        reply_to=doc_dict.get('reply_to'),\n",
    "                        mentions=doc_dict.get('mentions', [])\n",
    "                    )\n",
    "                    doc = self.preprocess(doc)\n",
    "                    results.append(doc)\n",
    "                    METRICS.increment('documents_ingested')\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to ingest document: {e}\")\n",
    "                    METRICS.increment('ingestion_errors')\n",
    "            return results\n",
    "\n",
    "\n",
    "# Standard preprocessors\n",
    "def clean_text(doc: ReputationDocument) -> ReputationDocument:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    import re\n",
    "    text = doc.text\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    doc.text = text\n",
    "    return doc\n",
    "\n",
    "def extract_entities(doc: ReputationDocument) -> ReputationDocument:\n",
    "    \"\"\"Extract named entities (placeholder for NER).\"\"\"\n",
    "    # In production, use spaCy or similar\n",
    "    doc.metadata['entities'] = []\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering Layer\n",
    "\n",
    "Modular feature extraction with caching and parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(ABC):\n",
    "    \"\"\"Abstract base class for feature extractors.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def output_dim(self) -> int:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract(self, doc: ReputationDocument) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    def extract_batch(self, docs: List[ReputationDocument]) -> np.ndarray:\n",
    "        \"\"\"Extract features for a batch (can be overridden for efficiency).\"\"\"\n",
    "        return np.vstack([self.extract(doc) for doc in docs])\n",
    "\n",
    "\n",
    "class LexiconFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"Extract lexicon-based sentiment features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.positive_words = {'good', 'great', 'excellent', 'amazing', 'love', 'best', 'happy'}\n",
    "        self.negative_words = {'bad', 'terrible', 'awful', 'hate', 'worst', 'disappointed'}\n",
    "        \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'lexicon'\n",
    "    \n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return 5  # [pos_ratio, neg_ratio, sentiment_score, word_count, avg_word_len]\n",
    "    \n",
    "    def extract(self, doc: ReputationDocument) -> np.ndarray:\n",
    "        tokens = doc.text.lower().split()\n",
    "        n = len(tokens) if tokens else 1\n",
    "        \n",
    "        pos_count = sum(1 for t in tokens if t in self.positive_words)\n",
    "        neg_count = sum(1 for t in tokens if t in self.negative_words)\n",
    "        \n",
    "        return np.array([\n",
    "            pos_count / n,\n",
    "            neg_count / n,\n",
    "            (pos_count - neg_count) / n,\n",
    "            np.log1p(n),\n",
    "            np.mean([len(t) for t in tokens]) if tokens else 0\n",
    "        ])\n",
    "\n",
    "\n",
    "class EmbeddingFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"Extract neural embedding features (simulated).\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 256):\n",
    "        self.embed_dim = embed_dim\n",
    "        # In production, load actual embedding model\n",
    "        self._vocab = {}\n",
    "        \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'embedding'\n",
    "    \n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return self.embed_dim\n",
    "    \n",
    "    def extract(self, doc: ReputationDocument) -> np.ndarray:\n",
    "        # Simulated embedding (in production, use transformer model)\n",
    "        np.random.seed(hash(doc.text) % 2**32)\n",
    "        return np.random.randn(self.embed_dim).astype(np.float32)\n",
    "\n",
    "\n",
    "class TemporalFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"Extract temporal features.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'temporal'\n",
    "    \n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return 7  # [hour_sin, hour_cos, dow_sin, dow_cos, is_weekend, month_sin, month_cos]\n",
    "    \n",
    "    def extract(self, doc: ReputationDocument) -> np.ndarray:\n",
    "        ts = doc.timestamp\n",
    "        hour = ts.hour\n",
    "        dow = ts.weekday()\n",
    "        month = ts.month\n",
    "        \n",
    "        return np.array([\n",
    "            np.sin(2 * np.pi * hour / 24),\n",
    "            np.cos(2 * np.pi * hour / 24),\n",
    "            np.sin(2 * np.pi * dow / 7),\n",
    "            np.cos(2 * np.pi * dow / 7),\n",
    "            float(dow >= 5),\n",
    "            np.sin(2 * np.pi * month / 12),\n",
    "            np.cos(2 * np.pi * month / 12)\n",
    "        ])\n",
    "\n",
    "\n",
    "class FeatureEngineeringPipeline:\n",
    "    \"\"\"Orchestrate feature extraction with caching and parallelization.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.extractors: List[FeatureExtractor] = []\n",
    "        self.logger = logging.getLogger('FeatureEngineering')\n",
    "        \n",
    "    def add_extractor(self, extractor: FeatureExtractor):\n",
    "        \"\"\"Register a feature extractor.\"\"\"\n",
    "        self.extractors.append(extractor)\n",
    "        self.logger.info(f\"Added extractor: {extractor.name} (dim={extractor.output_dim})\")\n",
    "        \n",
    "    @property\n",
    "    def total_dim(self) -> int:\n",
    "        return sum(e.output_dim for e in self.extractors)\n",
    "    \n",
    "    def extract(self, doc: ReputationDocument, use_cache: bool = True) -> np.ndarray:\n",
    "        \"\"\"Extract all features for a document.\"\"\"\n",
    "        cache_key = f\"features:{doc.id}\"\n",
    "        \n",
    "        if use_cache and self.config.cache_enabled:\n",
    "            cached = CACHE.get(cache_key)\n",
    "            if cached is not None:\n",
    "                return cached\n",
    "        \n",
    "        features = []\n",
    "        for extractor in self.extractors:\n",
    "            with METRICS.time(f'feature_{extractor.name}'):\n",
    "                feat = extractor.extract(doc)\n",
    "                features.append(feat)\n",
    "        \n",
    "        result = np.concatenate(features)\n",
    "        \n",
    "        if use_cache and self.config.cache_enabled:\n",
    "            CACHE.set(cache_key, result)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def extract_batch(self, docs: List[ReputationDocument], \n",
    "                      parallel: bool = True) -> np.ndarray:\n",
    "        \"\"\"Extract features for a batch of documents.\"\"\"\n",
    "        with METRICS.time('feature_extraction_batch'):\n",
    "            if parallel and len(docs) > 10:\n",
    "                with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                    features = list(executor.map(self.extract, docs))\n",
    "            else:\n",
    "                features = [self.extract(doc) for doc in docs]\n",
    "            \n",
    "            METRICS.increment('features_extracted', len(docs))\n",
    "            return np.vstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Layer - Ensemble Architecture\n",
    "\n",
    "Modular model components with ensemble prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComponent(ABC):\n",
    "    \"\"\"Abstract base class for ML model components.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, features: np.ndarray) -> Dict[str, Any]:\n",
    "        pass\n",
    "    \n",
    "    def predict_batch(self, features: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Batch prediction (can be overridden).\"\"\"\n",
    "        return [self.predict(f) for f in features]\n",
    "\n",
    "\n",
    "class SentimentModel(ModelComponent):\n",
    "    \"\"\"Sentiment classification model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 268, hidden_dim: int = 128):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)  # neg, neu, pos\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'sentiment'\n",
    "    \n",
    "    def predict(self, features: np.ndarray) -> Dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "            logits = self.model(x)\n",
    "            probs = F.softmax(logits, dim=-1).squeeze().numpy()\n",
    "            \n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        pred_idx = np.argmax(probs)\n",
    "        \n",
    "        return {\n",
    "            'label': labels[pred_idx],\n",
    "            'confidence': float(probs[pred_idx]),\n",
    "            'probabilities': {l: float(p) for l, p in zip(labels, probs)}\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, features: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(features, dtype=torch.float32)\n",
    "            logits = self.model(x)\n",
    "            probs = F.softmax(logits, dim=-1).numpy()\n",
    "        \n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        results = []\n",
    "        for p in probs:\n",
    "            pred_idx = np.argmax(p)\n",
    "            results.append({\n",
    "                'label': labels[pred_idx],\n",
    "                'confidence': float(p[pred_idx]),\n",
    "                'probabilities': {l: float(prob) for l, prob in zip(labels, p)}\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "class AnomalyModel(ModelComponent):\n",
    "    \"\"\"Anomaly detection model using reconstruction error.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 268, latent_dim: int = 32):\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'anomaly'\n",
    "    \n",
    "    def predict(self, features: np.ndarray) -> Dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "            z = self.encoder(x)\n",
    "            x_recon = self.decoder(z)\n",
    "            recon_error = F.mse_loss(x_recon, x).item()\n",
    "            \n",
    "        # Normalize score to [0, 1]\n",
    "        anomaly_score = min(recon_error / 2.0, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'is_anomaly': anomaly_score > self.threshold,\n",
    "            'anomaly_score': float(anomaly_score),\n",
    "            'reconstruction_error': float(recon_error)\n",
    "        }\n",
    "\n",
    "\n",
    "class ReputationScoreModel(ModelComponent):\n",
    "    \"\"\"Aggregate reputation score predictor.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 268):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Score in [0, 1]\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'reputation_score'\n",
    "    \n",
    "    def predict(self, features: np.ndarray) -> Dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "            score = self.model(x).item()\n",
    "            \n",
    "        return {\n",
    "            'reputation_score': float(score),\n",
    "            'category': 'positive' if score > 0.6 else ('negative' if score < 0.4 else 'neutral')\n",
    "        }\n",
    "\n",
    "\n",
    "class ModelEnsemble:\n",
    "    \"\"\"Ensemble of model components with weighted aggregation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.models: Dict[str, Tuple[ModelComponent, float]] = {}\n",
    "        self.logger = logging.getLogger('ModelEnsemble')\n",
    "        \n",
    "    def add_model(self, model: ModelComponent, weight: float = 1.0):\n",
    "        \"\"\"Add a model to the ensemble.\"\"\"\n",
    "        self.models[model.name] = (model, weight)\n",
    "        self.logger.info(f\"Added model: {model.name} (weight={weight})\")\n",
    "        \n",
    "    def predict(self, features: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Run all models and aggregate predictions.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, (model, weight) in self.models.items():\n",
    "            with METRICS.time(f'model_{name}'):\n",
    "                try:\n",
    "                    pred = model.predict(features)\n",
    "                    results[name] = pred\n",
    "                    METRICS.increment(f'model_{name}_success')\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Model {name} failed: {e}\")\n",
    "                    METRICS.increment(f'model_{name}_error')\n",
    "                    results[name] = {'error': str(e)}\n",
    "        \n",
    "        # Aggregate sentiment probabilities\n",
    "        if 'sentiment' in results and 'error' not in results['sentiment']:\n",
    "            results['aggregated_sentiment'] = results['sentiment']['label']\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def predict_batch(self, features: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Batch prediction.\"\"\"\n",
    "        n = features.shape[0]\n",
    "        all_results = [{} for _ in range(n)]\n",
    "        \n",
    "        for name, (model, weight) in self.models.items():\n",
    "            with METRICS.time(f'model_{name}_batch'):\n",
    "                try:\n",
    "                    preds = model.predict_batch(features)\n",
    "                    for i, pred in enumerate(preds):\n",
    "                        all_results[i][name] = pred\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Batch prediction failed for {name}: {e}\")\n",
    "                    for i in range(n):\n",
    "                        all_results[i][name] = {'error': str(e)}\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Anomaly & Drift Detection Layer\n",
    "\n",
    "Real-time monitoring for reputation anomalies and concept drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriftDetector:\n",
    "    \"\"\"Online drift detection using CUSUM algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 5.0, sensitivity: float = 0.1):\n",
    "        self.threshold = threshold\n",
    "        self.sensitivity = sensitivity\n",
    "        self.cusum_pos = 0.0\n",
    "        self.cusum_neg = 0.0\n",
    "        self.baseline_mean = None\n",
    "        self.baseline_std = None\n",
    "        self.warmup_buffer = []\n",
    "        self.warmup_size = 50\n",
    "        self.drift_points = []\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, value: float) -> Dict[str, Any]:\n",
    "        \"\"\"Update detector with new observation.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Warmup phase\n",
    "        if len(self.warmup_buffer) < self.warmup_size:\n",
    "            self.warmup_buffer.append(value)\n",
    "            if len(self.warmup_buffer) == self.warmup_size:\n",
    "                self.baseline_mean = np.mean(self.warmup_buffer)\n",
    "                self.baseline_std = np.std(self.warmup_buffer) + 1e-6\n",
    "            return {'drift_detected': False, 'in_warmup': True}\n",
    "        \n",
    "        # Normalize\n",
    "        z = (value - self.baseline_mean) / self.baseline_std\n",
    "        \n",
    "        # Update CUSUM\n",
    "        self.cusum_pos = max(0, self.cusum_pos + z - self.sensitivity)\n",
    "        self.cusum_neg = max(0, self.cusum_neg - z - self.sensitivity)\n",
    "        \n",
    "        # Check for drift\n",
    "        drift_detected = self.cusum_pos > self.threshold or self.cusum_neg > self.threshold\n",
    "        \n",
    "        if drift_detected:\n",
    "            self.drift_points.append(self.t)\n",
    "            self.cusum_pos = 0.0\n",
    "            self.cusum_neg = 0.0\n",
    "            self.warmup_buffer = [value]  # Reset warmup\n",
    "            self.baseline_mean = None\n",
    "            \n",
    "        return {\n",
    "            'drift_detected': drift_detected,\n",
    "            'cusum_pos': self.cusum_pos,\n",
    "            'cusum_neg': self.cusum_neg,\n",
    "            'z_score': z,\n",
    "            'timestamp': self.t\n",
    "        }\n",
    "\n",
    "\n",
    "class ReputationMonitor:\n",
    "    \"\"\"Monitor reputation metrics and detect anomalies.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.sentiment_drift = DriftDetector(\n",
    "            threshold=5.0, \n",
    "            sensitivity=config.drift_sensitivity\n",
    "        )\n",
    "        self.volume_drift = DriftDetector(threshold=5.0)\n",
    "        self.history = deque(maxlen=1000)\n",
    "        self.alerts: List[Dict] = []\n",
    "        self.logger = logging.getLogger('ReputationMonitor')\n",
    "        \n",
    "    def update(self, predictions: Dict[str, Any], doc: ReputationDocument) -> Dict[str, Any]:\n",
    "        \"\"\"Update monitor with new prediction.\"\"\"\n",
    "        result = {'alerts': []}\n",
    "        \n",
    "        # Track sentiment\n",
    "        if 'sentiment' in predictions and 'probabilities' in predictions['sentiment']:\n",
    "            # Compute sentiment score: -1 to +1\n",
    "            probs = predictions['sentiment']['probabilities']\n",
    "            sentiment_score = probs.get('positive', 0) - probs.get('negative', 0)\n",
    "            \n",
    "            drift_result = self.sentiment_drift.update(sentiment_score)\n",
    "            result['sentiment_drift'] = drift_result\n",
    "            \n",
    "            if drift_result.get('drift_detected') and self.config.alert_on_drift:\n",
    "                alert = {\n",
    "                    'type': 'SENTIMENT_DRIFT',\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'details': drift_result\n",
    "                }\n",
    "                self.alerts.append(alert)\n",
    "                result['alerts'].append(alert)\n",
    "                self.logger.warning(f\"Sentiment drift detected at t={drift_result['timestamp']}\")\n",
    "        \n",
    "        # Track anomalies\n",
    "        if 'anomaly' in predictions:\n",
    "            if predictions['anomaly'].get('is_anomaly') and self.config.alert_on_anomaly:\n",
    "                alert = {\n",
    "                    'type': 'ANOMALY_DETECTED',\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'document_id': doc.id,\n",
    "                    'score': predictions['anomaly'].get('anomaly_score')\n",
    "                }\n",
    "                self.alerts.append(alert)\n",
    "                result['alerts'].append(alert)\n",
    "        \n",
    "        # Store in history\n",
    "        self.history.append({\n",
    "            'timestamp': doc.timestamp,\n",
    "            'predictions': predictions,\n",
    "            'doc_id': doc.id\n",
    "        })\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Output & Action Layer\n",
    "\n",
    "Generate actionable outputs and integrate with downstream systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReputationAnalysisResult:\n",
    "    \"\"\"Complete analysis result for a document.\"\"\"\n",
    "    document_id: str\n",
    "    timestamp: datetime\n",
    "    sentiment: Dict[str, Any]\n",
    "    anomaly: Dict[str, Any]\n",
    "    reputation_score: float\n",
    "    monitoring: Dict[str, Any]\n",
    "    processing_time_ms: float\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'document_id': self.document_id,\n",
    "            'timestamp': self.timestamp.isoformat(),\n",
    "            'sentiment': self.sentiment,\n",
    "            'anomaly': self.anomaly,\n",
    "            'reputation_score': self.reputation_score,\n",
    "            'monitoring': self.monitoring,\n",
    "            'processing_time_ms': self.processing_time_ms\n",
    "        }\n",
    "\n",
    "\n",
    "class ActionHandler(ABC):\n",
    "    \"\"\"Abstract base for action handlers.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def handle(self, result: ReputationAnalysisResult) -> bool:\n",
    "        pass\n",
    "\n",
    "\n",
    "class AlertActionHandler(ActionHandler):\n",
    "    \"\"\"Handle alerts (log, notify, etc.).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger('AlertHandler')\n",
    "        \n",
    "    def handle(self, result: ReputationAnalysisResult) -> bool:\n",
    "        alerts = result.monitoring.get('alerts', [])\n",
    "        for alert in alerts:\n",
    "            self.logger.warning(f\"ALERT [{alert['type']}]: {alert}\")\n",
    "            # In production: send to Slack, PagerDuty, etc.\n",
    "        return True\n",
    "\n",
    "\n",
    "class DatabaseActionHandler(ActionHandler):\n",
    "    \"\"\"Persist results to database.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buffer: List[Dict] = []\n",
    "        self.buffer_size = 100\n",
    "        \n",
    "    def handle(self, result: ReputationAnalysisResult) -> bool:\n",
    "        self.buffer.append(result.to_dict())\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self._flush()\n",
    "        return True\n",
    "    \n",
    "    def _flush(self):\n",
    "        # In production: batch insert to database\n",
    "        METRICS.increment('db_writes', len(self.buffer))\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "class OutputPipeline:\n",
    "    \"\"\"Orchestrate output and actions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.handlers: List[ActionHandler] = []\n",
    "        \n",
    "    def add_handler(self, handler: ActionHandler):\n",
    "        self.handlers.append(handler)\n",
    "        \n",
    "    def process(self, result: ReputationAnalysisResult):\n",
    "        for handler in self.handlers:\n",
    "            try:\n",
    "                handler.handle(result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Handler {type(handler).__name__} failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Master Pipeline Orchestrator\n",
    "\n",
    "Unified pipeline that orchestrates all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReputationManagementPipeline:\n",
    "    \"\"\"\n",
    "    Master pipeline orchestrating all components for reputation management.\n",
    "    \n",
    "    Architecture:\n",
    "    ┌──────────────────────────────────────────────────────────────┐\n",
    "    │                    ReputationManagementPipeline              │\n",
    "    ├──────────────────────────────────────────────────────────────┤\n",
    "    │  Input ──▶ Ingestion ──▶ Features ──▶ Models ──▶ Monitor ──▶ Output │\n",
    "    └──────────────────────────────────────────────────────────────┘\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[PipelineConfig] = None):\n",
    "        self.config = config or PipelineConfig()\n",
    "        self.config.validate()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.ingestion = DataIngestionPipeline(self.config)\n",
    "        self.features = FeatureEngineeringPipeline(self.config)\n",
    "        self.ensemble = ModelEnsemble(self.config)\n",
    "        self.monitor = ReputationMonitor(self.config)\n",
    "        self.output = OutputPipeline()\n",
    "        \n",
    "        self.logger = logging.getLogger('ReputationPipeline')\n",
    "        self._setup_default_components()\n",
    "        \n",
    "    def _setup_default_components(self):\n",
    "        \"\"\"Configure default pipeline components.\"\"\"\n",
    "        # Preprocessors\n",
    "        self.ingestion.add_preprocessor(clean_text)\n",
    "        self.ingestion.add_preprocessor(extract_entities)\n",
    "        \n",
    "        # Feature extractors\n",
    "        self.features.add_extractor(LexiconFeatureExtractor())\n",
    "        self.features.add_extractor(EmbeddingFeatureExtractor(embed_dim=256))\n",
    "        self.features.add_extractor(TemporalFeatureExtractor())\n",
    "        \n",
    "        # Models\n",
    "        input_dim = self.features.total_dim\n",
    "        self.ensemble.add_model(SentimentModel(input_dim=input_dim), weight=1.0)\n",
    "        self.ensemble.add_model(AnomalyModel(input_dim=input_dim), weight=1.0)\n",
    "        self.ensemble.add_model(ReputationScoreModel(input_dim=input_dim), weight=1.0)\n",
    "        \n",
    "        # Action handlers\n",
    "        self.output.add_handler(AlertActionHandler())\n",
    "        self.output.add_handler(DatabaseActionHandler())\n",
    "        \n",
    "        self.logger.info(f\"Pipeline initialized with {self.features.total_dim} features\")\n",
    "        \n",
    "    def process_document(self, doc_dict: Dict) -> ReputationAnalysisResult:\n",
    "        \"\"\"Process a single document through the entire pipeline.\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Ingest\n",
    "        docs = self.ingestion.ingest_batch([doc_dict])\n",
    "        if not docs:\n",
    "            raise ValueError(\"Document ingestion failed\")\n",
    "        doc = docs[0]\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.features.extract(doc)\n",
    "        \n",
    "        # Run models\n",
    "        predictions = self.ensemble.predict(features)\n",
    "        \n",
    "        # Monitor\n",
    "        monitoring = self.monitor.update(predictions, doc)\n",
    "        \n",
    "        # Create result\n",
    "        processing_time = (time.perf_counter() - start_time) * 1000\n",
    "        \n",
    "        result = ReputationAnalysisResult(\n",
    "            document_id=doc.id,\n",
    "            timestamp=doc.timestamp,\n",
    "            sentiment=predictions.get('sentiment', {}),\n",
    "            anomaly=predictions.get('anomaly', {}),\n",
    "            reputation_score=predictions.get('reputation_score', {}).get('reputation_score', 0.5),\n",
    "            monitoring=monitoring,\n",
    "            processing_time_ms=processing_time\n",
    "        )\n",
    "        \n",
    "        # Output actions\n",
    "        self.output.process(result)\n",
    "        \n",
    "        METRICS.record('processing_time_ms', processing_time)\n",
    "        METRICS.increment('documents_processed')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_batch(self, doc_dicts: List[Dict], \n",
    "                      parallel: bool = True) -> List[ReputationAnalysisResult]:\n",
    "        \"\"\"Process a batch of documents.\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Ingest all\n",
    "        docs = self.ingestion.ingest_batch(doc_dicts)\n",
    "        if not docs:\n",
    "            return []\n",
    "        \n",
    "        # Extract features in parallel\n",
    "        features = self.features.extract_batch(docs, parallel=parallel)\n",
    "        \n",
    "        # Batch prediction\n",
    "        all_predictions = self.ensemble.predict_batch(features)\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        for doc, predictions in zip(docs, all_predictions):\n",
    "            monitoring = self.monitor.update(predictions, doc)\n",
    "            \n",
    "            result = ReputationAnalysisResult(\n",
    "                document_id=doc.id,\n",
    "                timestamp=doc.timestamp,\n",
    "                sentiment=predictions.get('sentiment', {}),\n",
    "                anomaly=predictions.get('anomaly', {}),\n",
    "                reputation_score=predictions.get('reputation_score', {}).get('reputation_score', 0.5),\n",
    "                monitoring=monitoring,\n",
    "                processing_time_ms=0  # Set below\n",
    "            )\n",
    "            results.append(result)\n",
    "            self.output.process(result)\n",
    "        \n",
    "        total_time = (time.perf_counter() - start_time) * 1000\n",
    "        avg_time = total_time / len(results) if results else 0\n",
    "        \n",
    "        for r in results:\n",
    "            r.processing_time_ms = avg_time\n",
    "        \n",
    "        METRICS.record('batch_processing_time_ms', total_time)\n",
    "        METRICS.increment('documents_processed', len(results))\n",
    "        \n",
    "        self.logger.info(f\"Processed {len(results)} documents in {total_time:.2f}ms\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def process_stream(self, doc_generator, max_buffer: int = 100):\n",
    "        \"\"\"Process documents from an async stream.\"\"\"\n",
    "        buffer = []\n",
    "        \n",
    "        async for doc_dict in doc_generator:\n",
    "            buffer.append(doc_dict)\n",
    "            \n",
    "            if len(buffer) >= max_buffer:\n",
    "                results = self.process_batch(buffer)\n",
    "                buffer.clear()\n",
    "                yield results\n",
    "        \n",
    "        # Process remaining\n",
    "        if buffer:\n",
    "            results = self.process_batch(buffer)\n",
    "            yield results\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get pipeline metrics.\"\"\"\n",
    "        metrics = METRICS.get_summary()\n",
    "        metrics['cache_hit_rate'] = CACHE.hit_rate\n",
    "        metrics['alerts'] = len(self.monitor.alerts)\n",
    "        metrics['drift_points'] = len(self.monitor.sentiment_drift.drift_points)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Pipeline Demonstration\n",
    "\n",
    "Test the complete pipeline with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = ReputationManagementPipeline()\n",
    "\n",
    "# Generate synthetic documents\n",
    "def generate_synthetic_documents(n: int = 100) -> List[Dict]:\n",
    "    \"\"\"Generate synthetic reputation documents.\"\"\"\n",
    "    templates = {\n",
    "        'positive': [\n",
    "            \"Great product! Absolutely love it. Best purchase ever.\",\n",
    "            \"Excellent customer service. Very helpful and professional.\",\n",
    "            \"Amazing quality. Would definitely recommend to everyone.\"\n",
    "        ],\n",
    "        'negative': [\n",
    "            \"Terrible experience. The product broke after one day.\",\n",
    "            \"Worst customer service ever. Waited hours for nothing.\",\n",
    "            \"Complete waste of money. Very disappointed.\"\n",
    "        ],\n",
    "        'neutral': [\n",
    "            \"The product is okay. Nothing special but works.\",\n",
    "            \"Average experience. Some good, some bad.\",\n",
    "            \"It does what it says. Not great, not terrible.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    docs = []\n",
    "    for i in range(n):\n",
    "        # Normal distribution of sentiments\n",
    "        if i < n * 0.8:\n",
    "            category = np.random.choice(['positive', 'neutral', 'negative'], p=[0.5, 0.35, 0.15])\n",
    "        else:\n",
    "            # Simulate a crisis period\n",
    "            category = np.random.choice(['positive', 'neutral', 'negative'], p=[0.1, 0.2, 0.7])\n",
    "        \n",
    "        text = np.random.choice(templates[category])\n",
    "        \n",
    "        docs.append({\n",
    "            'id': f'doc_{i}',\n",
    "            'text': text,\n",
    "            'source': 'social_media',\n",
    "            'timestamp': datetime.now(),\n",
    "            'author_id': f'user_{np.random.randint(1000)}'\n",
    "        })\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Generate test data\n",
    "test_docs = generate_synthetic_documents(200)\n",
    "print(f\"Generated {len(test_docs)} test documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process single document\n",
    "result = pipeline.process_document(test_docs[0])\n",
    "\n",
    "print(\"Single Document Analysis Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Document ID: {result.document_id}\")\n",
    "print(f\"Sentiment: {result.sentiment.get('label')} (confidence: {result.sentiment.get('confidence', 0):.3f})\")\n",
    "print(f\"Anomaly Score: {result.anomaly.get('anomaly_score', 0):.3f}\")\n",
    "print(f\"Reputation Score: {result.reputation_score:.3f}\")\n",
    "print(f\"Processing Time: {result.processing_time_ms:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batch\n",
    "batch_results = pipeline.process_batch(test_docs)\n",
    "\n",
    "print(f\"\\nProcessed {len(batch_results)} documents\")\n",
    "\n",
    "# Analyze results\n",
    "sentiments = [r.sentiment.get('label', 'unknown') for r in batch_results]\n",
    "sentiment_dist = {s: sentiments.count(s) for s in set(sentiments)}\n",
    "print(f\"Sentiment Distribution: {sentiment_dist}\")\n",
    "\n",
    "anomaly_count = sum(1 for r in batch_results if r.anomaly.get('is_anomaly', False))\n",
    "print(f\"Anomalies Detected: {anomaly_count}\")\n",
    "\n",
    "avg_rep_score = np.mean([r.reputation_score for r in batch_results])\n",
    "print(f\"Average Reputation Score: {avg_rep_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipeline metrics\n",
    "metrics = pipeline.get_metrics()\n",
    "\n",
    "print(\"\\nPipeline Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Documents Processed: {metrics['counters'].get('documents_processed', 0)}\")\n",
    "print(f\"Cache Hit Rate: {metrics['cache_hit_rate']:.2%}\")\n",
    "print(f\"Drift Points Detected: {metrics['drift_points']}\")\n",
    "print(f\"Alerts Generated: {metrics['alerts']}\")\n",
    "\n",
    "if 'processing_time_ms' in metrics:\n",
    "    print(f\"\\nProcessing Time (per doc):\")\n",
    "    print(f\"  Mean: {metrics['processing_time_ms']['mean']:.2f}ms\")\n",
    "    \n",
    "if 'batch_processing_time_ms' in metrics:\n",
    "    print(f\"\\nBatch Processing Time:\")\n",
    "    print(f\"  Mean: {metrics['batch_processing_time_ms']['mean']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "ax = axes[0, 0]\n",
    "colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red', 'unknown': 'blue'}\n",
    "ax.bar(sentiment_dist.keys(), sentiment_dist.values(), \n",
    "       color=[colors.get(s, 'blue') for s in sentiment_dist.keys()])\n",
    "ax.set_title('Sentiment Distribution')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Reputation score over time\n",
    "ax = axes[0, 1]\n",
    "scores = [r.reputation_score for r in batch_results]\n",
    "ax.plot(scores, alpha=0.7)\n",
    "ax.axhline(y=np.mean(scores), color='r', linestyle='--', label=f'Mean: {np.mean(scores):.3f}')\n",
    "ax.fill_between(range(len(scores)), 0.4, 0.6, alpha=0.2, color='gray', label='Neutral Zone')\n",
    "ax.set_title('Reputation Score Over Time')\n",
    "ax.set_xlabel('Document Index')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend()\n",
    "\n",
    "# Anomaly scores\n",
    "ax = axes[1, 0]\n",
    "anomaly_scores = [r.anomaly.get('anomaly_score', 0) for r in batch_results]\n",
    "ax.plot(anomaly_scores, alpha=0.7, color='orange')\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', label='Anomaly Threshold')\n",
    "ax.set_title('Anomaly Scores Over Time')\n",
    "ax.set_xlabel('Document Index')\n",
    "ax.set_ylabel('Anomaly Score')\n",
    "ax.legend()\n",
    "\n",
    "# Processing time\n",
    "ax = axes[1, 1]\n",
    "times = [r.processing_time_ms for r in batch_results]\n",
    "ax.hist(times, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=np.mean(times), color='r', linestyle='--', label=f'Mean: {np.mean(times):.2f}ms')\n",
    "ax.set_title('Processing Time Distribution')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Pipeline Architecture Summary\n",
    "\n",
    "### Key Design Patterns\n",
    "\n",
    "1. **Strategy Pattern**: Pluggable extractors, models, and handlers\n",
    "2. **Factory Pattern**: Configuration-driven component instantiation\n",
    "3. **Observer Pattern**: Monitoring and alerting system\n",
    "4. **Pipeline Pattern**: Sequential processing with clear stages\n",
    "5. **Decorator Pattern**: Metrics and caching wrappers\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "1. **Caching**: LRU cache with TTL for feature vectors\n",
    "2. **Batching**: Efficient batch processing with vectorized operations\n",
    "3. **Parallelization**: ThreadPoolExecutor for I/O-bound tasks\n",
    "4. **Lazy Loading**: On-demand model initialization\n",
    "5. **Early Stopping**: Short-circuit on anomalies\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "```python\n",
    "# Example production configuration\n",
    "prod_config = PipelineConfig(\n",
    "    max_workers=8,\n",
    "    async_enabled=True,\n",
    "    batch_size=128,\n",
    "    cache_enabled=True,\n",
    "    cache_ttl_seconds=7200,\n",
    "    alert_on_anomaly=True,\n",
    "    alert_on_drift=True\n",
    ")\n",
    "```\n",
    "\n",
    "### Extension Points\n",
    "\n",
    "- Add custom `FeatureExtractor` for domain-specific features\n",
    "- Implement new `ModelComponent` for specialized predictions\n",
    "- Create custom `ActionHandler` for integrations (Slack, PagerDuty, etc.)\n",
    "- Add preprocessing steps via `add_preprocessor()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
